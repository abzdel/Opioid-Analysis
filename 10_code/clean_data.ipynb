{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import xlrd\n",
    "import re\n",
    "import openpyxl\n",
    "import random\n",
    "\n",
    "# custom file that maps state names to abbreviations\n",
    "from abbreviation_conversion import abbrev_to_us_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up prescription data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the find_year function: 2019\n",
      "testing the find_month function: 12\n"
     ]
    }
   ],
   "source": [
    "def find_year(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: year\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "    \n",
    "    return int(TRANSACTION_DATE[-4:])\n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_year function: {find_year(12202019)}\")\n",
    "\n",
    "\n",
    "def find_month(TRANSACTION_DATE):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        TRANSACTION_DATE (str): date in format MMDDYYYY\n",
    "\n",
    "    Returns:\n",
    "        int: month\n",
    "    \"\"\"\n",
    "    TRANSACTION_DATE = str(TRANSACTION_DATE)\n",
    "\n",
    "    if len(TRANSACTION_DATE) == 8:\n",
    "        return int(TRANSACTION_DATE[:2])\n",
    "    else:\n",
    "        return int(TRANSACTION_DATE[:1])\n",
    "    \n",
    "\n",
    "# quick test \n",
    "print(f\"testing the find_month function: {find_month(12202019)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load in the data, we need to truncate the amount of columns we use as well as the states\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"QUANTITY\", \"TRANSACTION_DATE\"]\n",
    "\n",
    "# we know we need Florida, Texas, and Washington\n",
    "states = [\"FL\", \"TX\", \"WA\"]\n",
    "# since we are normalizing based on population, I think we should pick states that are regionally close to our target states\n",
    "# we can change this later as a group, but I have these selected below:\n",
    "\n",
    "# Florida comparison states\n",
    "fl_states = [\"PA\", \"MI\", \"NC\"]\n",
    "\n",
    "# Texas comparison states\n",
    "tx_states = [\"IL\", \"MA\", \"MI\"]\n",
    "\n",
    "# Washington comparison states\n",
    "wa_states = [\"NC\", \"CO\", \"MD\"]\n",
    "\n",
    "# create list of all states to use\n",
    "variable_states = []\n",
    "variable_states.extend(fl_states)\n",
    "variable_states.extend(tx_states)\n",
    "variable_states.extend(wa_states)\n",
    "\n",
    "# append variable states to our original list\n",
    "states.extend(variable_states)\n",
    "\n",
    "\n",
    "# create separate list of only florida and washington states for prescription data\n",
    "prescription_states = [state for state in states if state not in [\"IL\", \"MA\", \"MI\", \"TX\"]]\n",
    "\n",
    "# NC is appearing twice as it's a comparison state for both target states\n",
    "# making this a set will remove the duplicate\n",
    "prescription_states = list(set(prescription_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52032/1420671542.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# extract year out of date column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRANSACTION_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRANSACTION_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_month\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4770\u001b[0m         \"\"\"\n\u001b[1;32m-> 4771\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4773\u001b[0m     def _reduce(\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;31m# self.f is Callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1157\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52032/1420671542.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# extract year out of date column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRANSACTION_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRANSACTION_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_month\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\"]\n",
    "\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\", \"TRANSACTION_DATE\"]\n",
    "chunk_cols.extend(cols_to_keep)\n",
    "\n",
    "# now, load in our data as an iterator so we can load in chunks\n",
    "it = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv\", chunksize=200_000, sep='\\t', usecols=chunk_cols) # may have to change chunksize depending on your computer's memory\n",
    "\n",
    "# init empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for chunk in it:\n",
    "\n",
    "    # extract year out of date column\n",
    "    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "    chunk[\"month\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_month(x)))\n",
    "\n",
    "    # ensure dtypes for faster calculation below\n",
    "    chunk[\"MME_Conversion_Factor\"] = chunk[\"MME_Conversion_Factor\"].astype(float)\n",
    "    chunk[\"dos_str\"] = chunk[\"dos_str\"].astype(float)\n",
    "    chunk[\"DOSAGE_UNIT\"] = chunk[\"DOSAGE_UNIT\"].astype(float)\n",
    "\n",
    "    # calculate MME\n",
    "    chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "    chunk = chunk[cols_to_keep]\n",
    "\n",
    "    # ensure we're working in the correct date range\n",
    "    #filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "    #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "    # filter out the states we want\n",
    "    filtered_chunk = chunk[chunk[\"BUYER_STATE\"].isin(prescription_states)]\n",
    "    df = pd.concat([df, filtered_chunk])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year out of date column\n",
    "df[\"year\"] = df[\"TRANSACTION_DATE\"].apply(lambda x: find_year(x))\n",
    "df[\"month\"] = df[\"TRANSACTION_DATE\"].apply(lambda x: find_month(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in individual state prescription data\n",
    "\n",
    "The Washington Post article associated with our data states that data from 2013 and 2014 was only recently added. Resultingly, we found that it was missing from the large dataset of all states. However, upon further digging, we found that these years were present **on an individual state level**, so we will load these in and concatenate them with our larger dataframe above.\n",
    "\n",
    "Without chunking, the below takes 30 seconds for each file to load in. With chunking, this is reduced to about 4 seconds per record, so please make sure to leave this in its current format.\n",
    "\n",
    "\n",
    "\n",
    "current issues:\n",
    "- way more records for 2013 and 2014\n",
    "- non WA states not being read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TRANSACTION_DATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3803\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3804\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TRANSACTION_DATE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9376/2258716487.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# extract year out of date column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRANSACTION_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRANSACTION_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_month\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3804\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3805\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3806\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3807\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TRANSACTION_DATE'"
     ]
    }
   ],
   "source": [
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME\"]\n",
    "\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"]\n",
    "#chunk_cols.extend(cols_to_keep)\n",
    "#chunk_cols.remove(\"MME\") # MME is calculated in chunk, so we can't have it in this list\n",
    "\n",
    "# now, load in our data as an iterator so we can load in chunks\n",
    "it = pd.read_csv(\"00_source_data/arcos_all_washpost.tsv\", chunksize=200_000, sep='\\t', usecols=chunk_cols) # may have to change chunksize depending on your computer's memory\n",
    "\n",
    "# init empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for chunk in it:\n",
    "\n",
    "    # extract year out of date column\n",
    "    chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "    chunk[\"month\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_month(x)))\n",
    "\n",
    "    # ensure dtypes for faster calculation below\n",
    "    chunk[\"MME_Conversion_Factor\"] = chunk[\"MME_Conversion_Factor\"].astype(float)\n",
    "    chunk[\"dos_str\"] = chunk[\"dos_str\"].astype(float)\n",
    "    chunk[\"DOSAGE_UNIT\"] = chunk[\"DOSAGE_UNIT\"].astype(float)\n",
    "\n",
    "    # calculate MME\n",
    "    chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "    chunk = chunk[cols_to_keep]\n",
    "\n",
    "    # ensure we're working in the correct date range\n",
    "    #filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "    #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "    # filter out the states we want\n",
    "    filtered_chunk = chunk[chunk[\"BUYER_STATE\"].isin(prescription_states)]\n",
    "    df = pd.concat([df, filtered_chunk])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 00_source_data/state_prescriptions\\arcos-co-statewide-itemized.csv\n",
      "Finished processing 00_source_data/state_prescriptions\\arcos-fl-statewide-itemized.csv\n",
      "Finished processing 00_source_data/state_prescriptions\\arcos-md-statewide-itemized.csv\n",
      "Finished processing 00_source_data/state_prescriptions\\arcos-mi-statewide-itemized.csv\n",
      "Finished processing 00_source_data/state_prescriptions\\arcos-nc-statewide-itemized.csv\n",
      "Finished processing 00_source_data/state_prescriptions\\arcos-pa-statewide-itemized.csv\n",
      "Finished processing 00_source_data/state_prescriptions\\arcos-wa-statewide-itemized.csv\n"
     ]
    }
   ],
   "source": [
    "# set FINAL cols we want\n",
    "cols_to_keep = [\"BUYER_STATE\", \"BUYER_ZIP\", \"BUYER_COUNTY\", \"DRUG_CODE\", \"DRUG_NAME\", \"TRANSACTION_DATE\", \"MME\"]\n",
    "\n",
    "# set additional columns we need to calculate MME\n",
    "chunk_cols = [\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"]\n",
    "chunk_cols.extend(cols_to_keep)\n",
    "chunk_cols.remove(\"MME\") # MME is calculated in chunk, so we can't have it in this list\n",
    "\n",
    "\n",
    "cols_to_keep.extend([\"dos_str\", \"DOSAGE_UNIT\", \"MME_Conversion_Factor\"])\n",
    "\n",
    "path = r'00_source_data/state_prescriptions' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.csv\") # select all text files in folder\n",
    "\n",
    "assert len(filenames) == 7, \"There should be 7 files in the folder - check that we don't have a missing state\"\n",
    "\n",
    "df_prescriptions = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    it = pd.read_csv(f, chunksize=500_000, usecols = chunk_cols) # may have to change chunksize depending on your computer's memory\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "\n",
    "    for chunk in it:\n",
    "\n",
    "        # ensure dtypes for faster calculation below\n",
    "        chunk[\"MME_Conversion_Factor\"] = chunk[\"MME_Conversion_Factor\"].astype(float)\n",
    "        chunk[\"dos_str\"] = chunk[\"dos_str\"].astype(float)\n",
    "        chunk[\"DOSAGE_UNIT\"] = chunk[\"DOSAGE_UNIT\"].astype(float)\n",
    "\n",
    "        # calculate MME\n",
    "        chunk[\"MME\"] = chunk[\"dos_str\"] * chunk[\"MME_Conversion_Factor\"] * chunk[\"DOSAGE_UNIT\"]\n",
    "\n",
    "        chunk = chunk[cols_to_keep]\n",
    "        # ensure we're working in the correct date range\n",
    "        #filtered_chunk = chunk[chunk[\"year\"] > 2002]\n",
    "        #filtered_chunk = filtered_chunk[filtered_chunk[\"year\"] < 2016]\n",
    "\n",
    "        # extract year out of date column\n",
    "        chunk[\"year\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_year(x)))\n",
    "        chunk[\"month\"] = chunk[\"TRANSACTION_DATE\"].apply(lambda x: int(find_month(x)))\n",
    "\n",
    "        temp_df = pd.concat([temp_df, chunk])\n",
    "\n",
    "    print(f\"Finished processing {f}\")\n",
    "    df_prescriptions = pd.concat([df_prescriptions, temp_df], axis=0, ignore_index=True)\n",
    "\n",
    "df_prescriptions.drop(columns={\"TRANSACTION_DATE\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>MME</th>\n",
       "      <th>dos_str</th>\n",
       "      <th>DOSAGE_UNIT</th>\n",
       "      <th>MME_Conversion_Factor</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>CO</td>\n",
       "      <td>80127</td>\n",
       "      <td>JEFFERSON</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>CO</td>\n",
       "      <td>81004</td>\n",
       "      <td>PUEBLO</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2011</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>CO</td>\n",
       "      <td>81612</td>\n",
       "      <td>PITKIN</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2011</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>CO</td>\n",
       "      <td>80033</td>\n",
       "      <td>JEFFERSON</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>CO</td>\n",
       "      <td>80033</td>\n",
       "      <td>JEFFERSON</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2008</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56332042</th>\n",
       "      <td>WA</td>\n",
       "      <td>98408</td>\n",
       "      <td>PIERCE</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2007</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56332051</th>\n",
       "      <td>WA</td>\n",
       "      <td>98408</td>\n",
       "      <td>PIERCE</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56332061</th>\n",
       "      <td>WA</td>\n",
       "      <td>98408</td>\n",
       "      <td>PIERCE</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56334334</th>\n",
       "      <td>WA</td>\n",
       "      <td>98671</td>\n",
       "      <td>CLARK</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56334342</th>\n",
       "      <td>WA</td>\n",
       "      <td>98671</td>\n",
       "      <td>CLARK</td>\n",
       "      <td>9143</td>\n",
       "      <td>OXYCODONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76639 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BUYER_STATE  BUYER_ZIP BUYER_COUNTY  DRUG_CODE  DRUG_NAME  MME  \\\n",
       "72                CO      80127    JEFFERSON       9143  OXYCODONE  NaN   \n",
       "479               CO      81004       PUEBLO       9143  OXYCODONE  NaN   \n",
       "609               CO      81612       PITKIN       9143  OXYCODONE  NaN   \n",
       "1268              CO      80033    JEFFERSON       9143  OXYCODONE  NaN   \n",
       "1271              CO      80033    JEFFERSON       9143  OXYCODONE  NaN   \n",
       "...              ...        ...          ...        ...        ...  ...   \n",
       "56332042          WA      98408       PIERCE       9143  OXYCODONE  NaN   \n",
       "56332051          WA      98408       PIERCE       9143  OXYCODONE  NaN   \n",
       "56332061          WA      98408       PIERCE       9143  OXYCODONE  NaN   \n",
       "56334334          WA      98671        CLARK       9143  OXYCODONE  NaN   \n",
       "56334342          WA      98671        CLARK       9143  OXYCODONE  NaN   \n",
       "\n",
       "          dos_str  DOSAGE_UNIT  MME_Conversion_Factor  year  month  \n",
       "72            NaN        100.0                    1.5  2009      8  \n",
       "479           NaN        400.0                    1.5  2011     11  \n",
       "609           NaN        100.0                    1.5  2011      6  \n",
       "1268          NaN        200.0                    1.5  2008      9  \n",
       "1271          NaN        200.0                    1.5  2008     11  \n",
       "...           ...          ...                    ...   ...    ...  \n",
       "56332042      NaN        100.0                    1.5  2007      9  \n",
       "56332051      NaN        100.0                    1.5  2008      9  \n",
       "56332061      NaN        200.0                    1.5  2009     12  \n",
       "56334334      NaN        100.0                    1.5  2008      1  \n",
       "56334342      NaN        300.0                    1.5  2008      6  \n",
       "\n",
       "[76639 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions[df_prescriptions[\"MME\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "There should be no null values in the MME column",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24564/3218200145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"MME\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"There should be no null values in the MME column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: There should be no null values in the MME column"
     ]
    }
   ],
   "source": [
    "assert len(df_prescriptions[\"MME\"].isnull()) == 0, \"There should be no null values in the MME column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plan for this merge\n",
    "\n",
    "Loading in individual state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013    6966348\n",
       "2011    6830910\n",
       "2012    6802209\n",
       "2010    6609374\n",
       "2009    6211003\n",
       "2014    6069751\n",
       "2008    6009176\n",
       "2007    5666284\n",
       "2006    5172997\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing analysis, we learned that a handful of county values in MA were missing. However, when we looked up the associated zip codes (02401, 02174), we learned that we could fill these values with Plymouth and Middlesex Counties, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prescriptions[\"date_test\"] = df_prescriptions[\"TRANSACTION_DATE\"].apply(lambda x: datetime.strptime(str(x), \"%m%d%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prescriptions[\"year_test\"] = df_prescriptions[\"TRANSACTION_DATE\"].apply(lambda x: find_year(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prescriptions[\"TRANSACTION_DATE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2006, 2007, 2012, 2014, 2013, 2008, 2010, 2011, 2009], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prescriptions[df_prescriptions[\"BUYER_STATE\"] == \"WA\"].year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUYER_STATE</th>\n",
       "      <th>BUYER_ZIP</th>\n",
       "      <th>BUYER_COUNTY</th>\n",
       "      <th>DRUG_CODE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>MME</th>\n",
       "      <th>dos_str</th>\n",
       "      <th>DOSAGE_UNIT</th>\n",
       "      <th>MME_Conversion_Factor</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6702136</th>\n",
       "      <td>FL</td>\n",
       "      <td>34635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866573</th>\n",
       "      <td>FL</td>\n",
       "      <td>34635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7902916</th>\n",
       "      <td>FL</td>\n",
       "      <td>34635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8205321</th>\n",
       "      <td>FL</td>\n",
       "      <td>34635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9365143</th>\n",
       "      <td>FL</td>\n",
       "      <td>34635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9193</td>\n",
       "      <td>HYDROCODONE</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BUYER_STATE  BUYER_ZIP BUYER_COUNTY  DRUG_CODE    DRUG_NAME    MME  \\\n",
       "6702136          FL      34635          NaN       9193  HYDROCODONE  500.0   \n",
       "6866573          FL      34635          NaN       9193  HYDROCODONE  500.0   \n",
       "7902916          FL      34635          NaN       9193  HYDROCODONE  500.0   \n",
       "8205321          FL      34635          NaN       9193  HYDROCODONE  500.0   \n",
       "9365143          FL      34635          NaN       9193  HYDROCODONE  500.0   \n",
       "\n",
       "         dos_str  DOSAGE_UNIT  MME_Conversion_Factor  year  month  \n",
       "6702136      5.0        100.0                    1.0  2006      3  \n",
       "6866573      5.0        100.0                    1.0  2007      1  \n",
       "7902916      5.0        100.0                    1.0  2007      2  \n",
       "8205321      5.0        100.0                    1.0  2007      6  \n",
       "9365143      5.0        100.0                    1.0  2006     10  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at the null values (we checked, and these are all the values for which county is null)\n",
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values accordingly\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 2401, \"BUYER_COUNTY\"] = \"PLYMOUTH\"\n",
    "df_prescriptions.loc[df_prescriptions[\"BUYER_ZIP\"] == 2174, \"BUYER_COUNTY\"] = \"MIDDLESEX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "still have missing values for counties - double check code above",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24564/594678872.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"BUYER_COUNTY\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"BUYER_COUNTY\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"still have missing values for counties - double check code above\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: still have missing values for counties - double check code above"
     ]
    }
   ],
   "source": [
    "# check null values again\n",
    "df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]\n",
    "\n",
    "assert len(df_prescriptions[df_prescriptions[\"BUYER_COUNTY\"].isnull()]) == 0, \"still have missing values for counties - double check code above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'00_source_data/cause_of_death' # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.txt\") # select all text files in folder\n",
    "\n",
    "df = pd.DataFrame() # empty df - will store data from all txt files\n",
    "\n",
    "for f in filenames:\n",
    "    temp = pd.read_csv(f, index_col=None, header=0, sep='\\t')\n",
    "    # we're getting some extraneous notes at the bottom - let's just drop based on county as these will only be null for these useless notes columns\n",
    "    temp.dropna(subset={'County'}, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to separate county and state\n",
    "\n",
    "def abtract_state(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: state\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[1]\n",
    "\n",
    "\n",
    "\n",
    "def abstract_county(county):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        county (str): county name\n",
    "\n",
    "    Returns:\n",
    "        str: county\n",
    "    \"\"\"\n",
    "    return county.split(\", \")[0]\n",
    "\n",
    "# apply functions to our df\n",
    "df[\"State\"] = df.apply(lambda x: abtract_state(x[\"County\"]), axis=1)\n",
    "df[\"County\"] = df.apply(lambda x: abstract_county(x[\"County\"]), axis=1)\n",
    "\n",
    "# do not need notes column, let's just drop it here\n",
    "df.drop(columns={\"Notes\"}, inplace=True)\n",
    "\n",
    "df_cause_of_death = df.copy() # keep a copy of this df for later filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's filter our dataframe to be only the states we want\n",
    "df_cause_of_death = df_cause_of_death[df_cause_of_death[\"State\"].isin(states)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in County Population data\n",
    "\n",
    "[Census county pop. data, 2000-2010](https://www.census.gov/data/tables/time-series/demo/popest/intercensal-2000-2010-counties.html)<br>\n",
    "[Census county pop. data, 2010-2019](https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html)<br>\n",
    "For both, just select the appropriate states on the webpage. We will clean and merge as needed in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2000s data\n",
    "\n",
    "The way the 2000s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop unnamed 1, 12, and 13\n",
    "    - these contain redundant data about populations from specific dates\n",
    "    - Unnamed 12 is 2010s pop - will be redundant as our next dataset has this as well. Using the newer data\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>350888.0</td>\n",
       "      <td>359816.0</td>\n",
       "      <td>370753.0</td>\n",
       "      <td>377464.0</td>\n",
       "      <td>384809.0</td>\n",
       "      <td>395146.0</td>\n",
       "      <td>406575.0</td>\n",
       "      <td>415746.0</td>\n",
       "      <td>424913.0</td>\n",
       "      <td>435700.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>14954.0</td>\n",
       "      <td>14956.0</td>\n",
       "      <td>15114.0</td>\n",
       "      <td>15067.0</td>\n",
       "      <td>15217.0</td>\n",
       "      <td>15236.0</td>\n",
       "      <td>15196.0</td>\n",
       "      <td>15180.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>15289.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>491482.0</td>\n",
       "      <td>502393.0</td>\n",
       "      <td>508936.0</td>\n",
       "      <td>513690.0</td>\n",
       "      <td>518971.0</td>\n",
       "      <td>524466.0</td>\n",
       "      <td>531619.0</td>\n",
       "      <td>542039.0</td>\n",
       "      <td>552461.0</td>\n",
       "      <td>563161.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>10454.0</td>\n",
       "      <td>10885.0</td>\n",
       "      <td>11089.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>11496.0</td>\n",
       "      <td>11937.0</td>\n",
       "      <td>12262.0</td>\n",
       "      <td>12250.0</td>\n",
       "      <td>12169.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>4501.0</td>\n",
       "      <td>4471.0</td>\n",
       "      <td>4336.0</td>\n",
       "      <td>4117.0</td>\n",
       "      <td>4064.0</td>\n",
       "      <td>3997.0</td>\n",
       "      <td>3933.0</td>\n",
       "      <td>3866.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>3767.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2000      2001      2002      2003      2004  \\\n",
       "0      Adams County  350888.0  359816.0  370753.0  377464.0  384809.0   \n",
       "1    Alamosa County   14954.0   14956.0   15114.0   15067.0   15217.0   \n",
       "2   Arapahoe County  491482.0  502393.0  508936.0  513690.0  518971.0   \n",
       "3  Archuleta County   10020.0   10454.0   10885.0   11089.0   11266.0   \n",
       "4       Baca County    4501.0    4471.0    4336.0    4117.0    4064.0   \n",
       "\n",
       "       2005      2006      2007      2008      2009 State  \n",
       "0  395146.0  406575.0  415746.0  424913.0  435700.0    CO  \n",
       "1   15236.0   15196.0   15180.0   15300.0   15289.0    CO  \n",
       "2  524466.0  531619.0  542039.0  552461.0  563161.0    CO  \n",
       "3   11496.0   11937.0   12262.0   12250.0   12169.0    CO  \n",
       "4    3997.0    3933.0    3866.0    3806.0    3767.0    CO  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init emmpty df for our population data\n",
    "pops00 = pd.DataFrame()\n",
    "\n",
    "# end goal - add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2000s/\" # point to correct folder\n",
    "filenames = glob.glob(path + \"*.xls\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2000s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2000], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Unnamed: 1\", \"Unnamed: 12\", \"Unnamed: 13\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    pops00 = pd.concat([pops00, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops00.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide to cleaning - 2010s data\n",
    "\n",
    "The way the 2010s excel files are formatted, we can clean the data in the following way\n",
    "\n",
    "- load in with header=3\n",
    "- drop null on any of the populations\n",
    "    - notes at the bottom will be removed\n",
    "- drop census, estimates base\n",
    "- drop first row\n",
    "    - state as a whole\n",
    "- rename Unnamed: 0 to county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams County</td>\n",
       "      <td>443691.0</td>\n",
       "      <td>452201.0</td>\n",
       "      <td>460558.0</td>\n",
       "      <td>469978.0</td>\n",
       "      <td>479946.0</td>\n",
       "      <td>490443.0</td>\n",
       "      <td>497734.0</td>\n",
       "      <td>503590.0</td>\n",
       "      <td>511354.0</td>\n",
       "      <td>517421.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamosa County</td>\n",
       "      <td>15515.0</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>15680.0</td>\n",
       "      <td>15787.0</td>\n",
       "      <td>15803.0</td>\n",
       "      <td>15894.0</td>\n",
       "      <td>16053.0</td>\n",
       "      <td>16108.0</td>\n",
       "      <td>16248.0</td>\n",
       "      <td>16233.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arapahoe County</td>\n",
       "      <td>574747.0</td>\n",
       "      <td>585968.0</td>\n",
       "      <td>596500.0</td>\n",
       "      <td>608467.0</td>\n",
       "      <td>619034.0</td>\n",
       "      <td>630984.0</td>\n",
       "      <td>638950.0</td>\n",
       "      <td>644478.0</td>\n",
       "      <td>651797.0</td>\n",
       "      <td>656590.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Archuleta County</td>\n",
       "      <td>12046.0</td>\n",
       "      <td>12021.0</td>\n",
       "      <td>12132.0</td>\n",
       "      <td>12216.0</td>\n",
       "      <td>12231.0</td>\n",
       "      <td>12387.0</td>\n",
       "      <td>12825.0</td>\n",
       "      <td>13295.0</td>\n",
       "      <td>13730.0</td>\n",
       "      <td>14029.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baca County</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>3722.0</td>\n",
       "      <td>3656.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>3554.0</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>3581.0</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             County      2010      2011      2012      2013      2014  \\\n",
       "0      Adams County  443691.0  452201.0  460558.0  469978.0  479946.0   \n",
       "1    Alamosa County   15515.0   15709.0   15680.0   15787.0   15803.0   \n",
       "2   Arapahoe County  574747.0  585968.0  596500.0  608467.0  619034.0   \n",
       "3  Archuleta County   12046.0   12021.0   12132.0   12216.0   12231.0   \n",
       "4       Baca County    3807.0    3778.0    3722.0    3656.0    3587.0   \n",
       "\n",
       "       2015      2016      2017      2018      2019 State  \n",
       "0  490443.0  497734.0  503590.0  511354.0  517421.0    CO  \n",
       "1   15894.0   16053.0   16108.0   16248.0   16233.0    CO  \n",
       "2  630984.0  638950.0  644478.0  651797.0  656590.0    CO  \n",
       "3   12387.0   12825.0   13295.0   13730.0   14029.0    CO  \n",
       "4    3555.0    3530.0    3554.0    3584.0    3581.0    CO  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pops10 = pd.DataFrame()\n",
    "\n",
    "# add every excel file in 00_source_data/county_pop/2000s to pops00\n",
    "\n",
    "path = r\"00_source_data/county_pop/2010s\" # point to correct folder\n",
    "filenames = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "for f in filenames:\n",
    "\n",
    "    # read in current file with header = 3\n",
    "    temp = pd.read_excel(f, header = 3)\n",
    "\n",
    "    # regex to pull out state from filename\n",
    "    r = re.search(\"(2010s)(.)(\\w+)\", f)[3]\n",
    "    temp[\"State\"] = r[:2].upper()\n",
    "    \n",
    "    # drop null on any of the years\n",
    "    temp.dropna(subset=[2010], inplace=True)\n",
    "\n",
    "    #drop useless columns\n",
    "    temp.drop(columns={\"Census\", \"Estimates Base\"}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    temp = temp.iloc[1:, :]\n",
    "\n",
    "    # rename some cols\n",
    "    temp.rename(columns={\"Unnamed: 0\": \"County\"}, inplace=True)\n",
    "\n",
    "    # remove period at beginning of each county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x[1:])\n",
    "\n",
    "    # strip state from county\n",
    "    temp[\"County\"] = temp[\"County\"].apply(lambda x: x.split(\", \")[0])\n",
    "\n",
    "    pops10 = pd.concat([pops10, temp], axis=0, ignore_index=True)\n",
    "\n",
    "# quick peek at the data\n",
    "pops10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt both dfs to get tidy format\n",
    "pops00 = pops00.melt([\"County\", \"State\"])\n",
    "pops10 = pops10.melt([\"County\", \"State\"])\n",
    "\n",
    "# rename columns accordingly\n",
    "pops00.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "pops10.rename(columns={\"variable\": \"Year\", \"value\": \"Population\"}, inplace=True)\n",
    "\n",
    "# concatenate the two dfs to get all our population data in one place\n",
    "pops = pd.concat([pops00, pops10], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have the same number of counties between datasets\n",
    "assert len(pops00[\"County\"].unique()) == len(pops10[\"County\"].unique())\n",
    "\n",
    "# check that we have the same number of counties every year\n",
    "# first, create a df with the number of counties per year\n",
    "pops_county_check = pops.groupby([\"State\", \"Year\"])[\"County\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>county_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>CO</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>FL</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>IL</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>MA</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year State  county_count\n",
       "0  2000    CO            64\n",
       "1  2000    FL            67\n",
       "2  2000    IL           102\n",
       "3  2000    MA            14\n",
       "4  2000    MD            24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the sum of counties by year and state - will help us check if number of counties changes over the years\n",
    "grouped_states = pops_county_check.groupby([\"Year\", \"State\"])[\"County\"].sum().reset_index().rename(columns={\"County\": \"county_count\"})\n",
    "\n",
    "# here's what this looks like\n",
    "# we get a dataframe of states and years, with the number of counties in each state in each year\n",
    "grouped_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above query, we should be able to assert that the number of counties per year is the same\n",
    "# below statement should always equal zero\n",
    "\n",
    "assert (grouped_states.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "#assert (grouped_states10.duplicated(subset=[\"Year\", \"State\"]).sum() == 0)\n",
    "\n",
    "\n",
    "# ensure no duplicate values\n",
    "assert pops.duplicated().sum() == 0\n",
    "\n",
    "# loop to check that every state has the same number of counties every year\n",
    "for state in states:\n",
    "    assert (pops[pops[\"State\"] == state].Year.value_counts().nunique() == 1), f\"error on {state}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to integrate fip numbers for a better merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in fips data from external source\n",
    "fips = pd.read_csv(\"https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AL']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get key from value in our abbreviation dictionary\n",
    "# will help us have consistent formatting across dataframes for merging purposes\n",
    "def get_keys_from_value(d, val):\n",
    "    return [k for k, v in d.items() if v == val]\n",
    "\n",
    "\n",
    "keys = get_keys_from_value(abbrev_to_us_state, 'Alabama')\n",
    "keys # quick peek to make sure it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above to entire fips dataframe\n",
    "fips[\"state_abbrev\"] = fips[\"StateName\"].apply(lambda x: get_keys_from_value(abbrev_to_us_state, x)[0])\n",
    "\n",
    "# filter fips to appropriate states, now that it's in the correct format\n",
    "fips = fips[fips[\"state_abbrev\"].isin(states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further cleaning of values before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get rid of the word county in pop df\n",
    "def remove_county(x):\n",
    "\n",
    "    if \"County\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "pops[\"county_test\"] = pops[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# fix dona ana and la salle parish\n",
    "pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"DoÃ±a Ana\", \"Dona Ana\"))\n",
    "fips[\"CountyName\"] = fips[\"CountyName\"].apply(lambda x: x.replace(\"DoÃƒÂ±a Ana\", \"Dona Ana\"))\n",
    "\n",
    "\n",
    "#pops[\"county_test\"] = pops[\"county_test\"].apply(lambda x: x.replace(\"La Salle Parish\", \"La Salle\"))\n",
    "\n",
    "\n",
    "# rename county_test where state is texas and county is la salle to La Salle (TX)\n",
    "pops.loc[(pops[\"State\"] == \"TX\") & (pops[\"county_test\"] == \"La Salle\"), \"county_test\"] = \"La Salle County\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change La Salle county name in fips to La Salle County\n",
    "fips.loc[fips[\"CountyName\"] == \"La Salle\", \"CountyName\"] = \"La Salle County\"\n",
    "fips.loc[fips[\"CountyName\"] == \"LaSalle Parish\", \"CountyName\"] = \"La Salle Parish\"\n",
    "pops.loc[pops[\"county_test\"] == \"LaSalle Parish\", \"county_test\"] = \"La Salle Parish\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final merge for population dataset & fip number dataset\n",
    "pops_copy = pops.merge(fips[[\"state_abbrev\", \"CountyFIPS\", \"StateFIPS\", \"CountyName\"]], left_on=[\"county_test\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should never end up with anything left out of merge\n",
    "assert len(pops_copy[pops_copy[\"_merge\"] != \"both\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fip numbers to df_prescriptions\n",
    "\n",
    "# create copies of both dfs so we have a checkpoint to access our old dfs\n",
    "prescriptions_copy = df_prescriptions.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix nulls in prescriptions at this point\n",
    "\n",
    "prescriptions_copy.dropna(subset=[\"BUYER_COUNTY\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make buyer_county all lowercase\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.lower())\n",
    "\n",
    "# do the same for fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county and parish from fips_copy\n",
    "\n",
    "def remove_parish(x):\n",
    "\n",
    "    if \"parish\" in x:\n",
    "        return x[:-7]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# prescription dataset has similar format - match fips to this format\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_county(x))\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_parish(x))\n",
    "\n",
    "def expand_saint(x):\n",
    "\n",
    "    if \"st.\" in x:\n",
    "        return x.replace(\"st.\", \"saint\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# fix various other inconsistencies\n",
    "# left only values first\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: expand_saint(x))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"st john the baptist\", \"saint john the baptist\"))\n",
    "\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dekalb\", \"de kalb\"))\n",
    "\n",
    "# fix right only values\n",
    "\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"desoto\", \"de soto\"))\n",
    "\n",
    "\n",
    "\n",
    "# function to remove apostrophes from county names\n",
    "def remove_apostrophe(x):\n",
    "    \n",
    "    if \"'\" in x:\n",
    "        return x.replace(\"'\", \"\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# apply to fips\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: remove_apostrophe(x))\n",
    "\n",
    "# replace lasalle with la salle in fips copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"lasalle\", \"la salle\"))\n",
    "\n",
    "# replace dewitt with de witt in prescriptions copy\n",
    "prescriptions_copy[\"BUYER_COUNTY\"] = prescriptions_copy[\"BUYER_COUNTY\"].apply(lambda x: x.replace(\"dewitt\", \"de witt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fips = prescriptions_copy.merge(fips_copy, left_on=[\"BUYER_COUNTY\", \"BUYER_STATE\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)\n",
    "\n",
    "# capitalize year and month columns\n",
    "prescriptions_fips.rename(columns={\"year\": \"Year\", \"month\": \"Month\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011.0    651518\n",
       "2010.0    648770\n",
       "2012.0    627008\n",
       "2013.0    624624\n",
       "2009.0    617209\n",
       "2014.0    586696\n",
       "2008.0    585094\n",
       "2007.0    560391\n",
       "2006.0    521957\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STILL HAVE ALL WASHINGTON YEARS HERE\n",
    "prescriptions_fips[prescriptions_fips[\"BUYER_STATE\"] == \"WA\"].Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values\n",
    "\n",
    "Since we have plenty of values joined with right_only indicator status, we know that some counties in our FIPS dataset is not merging correctly to our prescriptions dataset. Let's take one example of missing data - San Juan County in Washington. When exploring the Washington Post's website on prescription data and selecting for this county individually, we can see that the data does, in fact, exist here. However, we see an exceptionally low rate of pills prescribed (32 pills per person per year, in this case). Upon looking at some other examples, we can see that the counties joining with right_only below are likely missing from the Washington Post data due to having such small numbers.\n",
    "\n",
    "Since any given missing county does not have data available, we need to find a way to impute these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips.Year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KING            1335946\n",
       "SNOHOMISH        677889\n",
       "PIERCE           564101\n",
       "SPOKANE          497385\n",
       "CLARK            326183\n",
       "YAKIMA           193659\n",
       "THURSTON         179964\n",
       "KITSAP           171125\n",
       "BENTON           161366\n",
       "WHATCOM          137868\n",
       "SKAGIT           123726\n",
       "COWLITZ          114371\n",
       "CLALLAM           86172\n",
       "LEWIS             82682\n",
       "GRAYS HARBOR      72342\n",
       "CHELAN            68918\n",
       "GRANT             66954\n",
       "WALLA WALLA       59081\n",
       "STEVENS           56279\n",
       "MASON             51781\n",
       "ISLAND            51027\n",
       "FRANKLIN          46031\n",
       "OKANOGAN          42039\n",
       "ASOTIN            31367\n",
       "KITTITAS          30180\n",
       "WHITMAN           29699\n",
       "DOUGLAS           29043\n",
       "PEND OREILLE      25907\n",
       "PACIFIC           21446\n",
       "JEFFERSON         17687\n",
       "SAN JUAN          14999\n",
       "ADAMS             14314\n",
       "LINCOLN           11403\n",
       "KLICKITAT          8168\n",
       "SKAMANIA           6280\n",
       "COLUMBIA           5254\n",
       "FERRY              5216\n",
       "WAHKIAKUM          3376\n",
       "GARFIELD           2039\n",
       "Name: BUYER_COUNTY, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at an example of our missing data\n",
    "wa = df_prescriptions[df_prescriptions[\"BUYER_STATE\"] == 'WA']\n",
    "\n",
    "wa[\"BUYER_COUNTY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prescriptions_fips[prescriptions_fips[\"_merge\"] == \"left_only\"].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to prescription dataset\n",
    "\n",
    "This could be okay, but I want to do a quick check that there are just not records for these counties. To do this, I'll take a small sample of counties in our FIPS dataset that did NOT merge properly to the prescriptions dataset, and search each one manually in the prescription dataset. I will search various different ways the counties could be transcribed, as well as google the county to ensure there are no secondary names for the same county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BUYER_STATE', 'BUYER_ZIP', 'BUYER_COUNTY', 'DRUG_CODE', 'DRUG_NAME',\n",
       "       'MME', 'dos_str', 'DOSAGE_UNIT', 'MME_Conversion_Factor', 'Year',\n",
       "       'Month', 'StateFIPS', 'CountyFIPS_3', 'CountyName', 'StateName',\n",
       "       'CountyFIPS', 'StateAbbr', 'STATE_COUNTY', 'state_abbrev', '_merge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_fips[prescriptions_fips[\"_merge\"] != \"both\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a new solution to the above - should impute the values somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] != \"right_only\"]\n",
    "\n",
    "# may have to add explanation for this higher up\n",
    "# re add assertion once value imputer is done\n",
    "#assert len(prescriptions_copy) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding fips to our cause of death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of both dfs\n",
    "\n",
    "cause_of_death_copy = df_cause_of_death.copy()\n",
    "fips_copy = fips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove county once again\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: remove_county(x))\n",
    "\n",
    "\n",
    "# clean some other miscellaneous values up\n",
    "\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"LaSalle Parish\", \"La Salle Parish\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"DeBaca\", \"De Baca\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle\", \"La Salle County\"))\n",
    "cause_of_death_copy[\"County\"] = cause_of_death_copy[\"County\"].apply(lambda x: x.replace(\"La Salle County Parish\", \"La Salle Parish\"))\n",
    "\n",
    "\n",
    "\n",
    "# expand mckean to mc kean in fips_copy\n",
    "fips_copy[\"CountyName\"] = fips_copy[\"CountyName\"].apply(lambda x: x.replace(\"McKean\", \"Mc Kean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_copy.merge(fips_copy, left_on=[\"County\", \"State\"], right_on=[\"CountyName\", \"state_abbrev\"], how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all counties joining back to cause of death dataset\n",
    "\n",
    "If the number of people in a given category (eg. one county/year/cause of death category) is less than 10, those records do not appear in this data. There is also a technicality in the number of total deaths vs. drug deaths (which we are interested in).\n",
    "\n",
    "The example we are given is that if a county has 20 deaths unrelated to drugs and alcohol, and only 7 related to alcohol, only the former figure will be reported. In the next notebook (pick_states.ipynb), we will filter by cause of death. In this notebook, since we still have all causes of death, we will impute for every missing value.\n",
    "\n",
    "To impute this data, we will fill in missing values with **a random integer from 0 to 9**. We thought of drawing from a normal distribution, but this implies negative values could be attained. We could take their absolute values to negate this effect, but then we are no longer drawing from a *true* normal distribution, so we chose to pick random values in our range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace null value with a random integer from 0 to 10 with a normal distribution\n",
    "def value_imputer(x):\n",
    "    if pd.isnull(x):\n",
    "        return random.randint(0, 9)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "cause_of_death_fips[\"Deaths\"] = cause_of_death_fips[\"Deaths\"].apply(lambda x: value_imputer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2\n",
       "0    1\n",
       "8    1\n",
       "6    1\n",
       "2    1\n",
       "Name: Deaths, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at our new imputed data\n",
    "cause_of_death_fips[cause_of_death_fips[\"_merge\"] != \"both\"].Deaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Population to final DataFrames\n",
    "\n",
    "For pop_fips, cause_of_death_fips, and prescription_fips. Steps needed:\n",
    "\n",
    "- Create unique ID from county FIPS and state FIPS\n",
    "- Merge population dataset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_fips = cause_of_death_fips[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "#pops_copy = pops_copy[cause_of_death_fips[\"_merge\"] == \"both\"]\n",
    "prescriptions_fips = prescriptions_fips[prescriptions_fips[\"_merge\"] == \"both\"]\n",
    "\n",
    "\n",
    "# drop merge columns\n",
    "cause_of_death_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "prescriptions_fips.drop(columns=[\"_merge\"], inplace=True)\n",
    "pops_copy.drop(columns=[\"_merge\",], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique FIP from county and state fips\n",
    "\n",
    "cause_of_death_fips[\"FIP_unique\"] = cause_of_death_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + cause_of_death_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "prescriptions_fips[\"FIP_unique\"] = prescriptions_fips[\"CountyFIPS\"].apply(lambda x: str(x)) + prescriptions_fips[\"StateFIPS\"].apply(lambda x: str(x))\n",
    "pops_copy[\"FIP_unique\"] = pops_copy[\"CountyFIPS\"].apply(lambda x: str(x)) + pops_copy[\"StateFIPS\"].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some sort of assert here. not sure what it should be yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final prescriptions dataset with populations\n",
    "# can safely left join here, because we only need records in the prescriptions dataset\n",
    "prescriptions = prescriptions_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert (prescriptions[\"_merge\"] == \"both\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011.0    651518\n",
       "2010.0    648770\n",
       "2012.0    627008\n",
       "2013.0    624624\n",
       "2009.0    617209\n",
       "2014.0    586696\n",
       "2008.0    585094\n",
       "2007.0    560391\n",
       "2006.0    521957\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STILL HAVE WASHINGTON YEARS HERE\n",
    "prescriptions[prescriptions[\"BUYER_STATE\"] == \"WA\"].Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more assert to check length\n",
    "assert len(prescriptions) == len(prescriptions_fips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "prescriptions.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County\", \"CountyFIPS_3\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "prescriptions.rename(columns={\"CountyName_x\": \"CountyName\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final cause of death dataset with populations\n",
    "# can safely left join here, because we only need records in the cause of death dataset\n",
    "cause_of_death = cause_of_death_fips.merge(pops_copy, on=[\"FIP_unique\", \"Year\"], how=\"left\", indicator=True)\n",
    "\n",
    "assert cause_of_death_fips.Deaths.isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some useless columns\n",
    "cause_of_death.drop(columns=[\"_merge\", \"CountyName_y\", \"StateFIPS_y\", \"CountyFIPS_y\",\"state_abbrev_y\", \"County_y\", \"CountyFIPS_3\", \"State_y\"], inplace=True)\n",
    "\n",
    "# rename x columns\n",
    "cause_of_death.rename(columns={\"County_x\": \"County\", \"Year_x\": \"Year\", \"State_x\": \"State\", \"StateFIPS_x\": \"StateFIPS\", \"CountyFIPS_x\": \"CountyFIPS\", \"state_abbrev_x\": \"state_abbrev\", \"CountyName_x\": \"CountyName\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9376/3788078211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cause_of_death\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause_of_death\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_prescriptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprescriptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# asserts to make sure we didn't lose any records from our original datasets\n",
    "\n",
    "assert len(df_cause_of_death) == len(cause_of_death)\n",
    "assert len(df_prescriptions) == len(prescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export main, unjoined datasets in case we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9376/111128345.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcause_of_death\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20_intermediate_files/cause_of_death_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprescriptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"20_intermediate_files/arcos_all_washpost_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3718\u001b[0m         )\n\u001b[0;32m   3719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3720\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         )\n\u001b[1;32m-> 1189\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m             )\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         libwriters.write_csv_rows(\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abzdel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cause_of_death.to_csv(\"20_intermediate_files/cause_of_death_clean.csv\", index=False)\n",
    "prescriptions.to_csv(\"20_intermediate_files/arcos_all_washpost_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final 3 datasets\n",
    "\n",
    "We should have: (UNSURE IF WE SHOULD EXTEND DATE RANGES, CURRENTLY 3 YEARS BEFORE AND AFTER POLICY IMPLEMENTATION)\n",
    "\n",
    "- Florida and Georgia 2007 - 2013\n",
    "- Texas and Oklahoma 2004 - 2010\n",
    "- Washington and Oregon 2009 - 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug overdose - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida and Georgia\n",
    "\n",
    "prescriptions_fl = prescriptions.copy()\n",
    "prescriptions_wa = prescriptions.copy()\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"BUYER_STATE\"] == \"FL\") | (prescriptions_fl[\"BUYER_STATE\"].isin(fl_states))]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"BUYER_STATE\"] == \"WA\") | (prescriptions_wa[\"BUYER_STATE\"]).isin(wa_states)]\n",
    "\n",
    "\n",
    "\n",
    "# filter appropriate years\n",
    "fl_start = 2007\n",
    "fl_end = 2013\n",
    "\n",
    "# tx will only be used for overdose deaths\n",
    "tx_start = 2004\n",
    "tx_end = 2010\n",
    "\n",
    "wa_start = 2009\n",
    "wa_end = 2015\n",
    "\n",
    "\n",
    "prescriptions_fl = prescriptions_fl[(prescriptions_fl[\"Year\"] >= fl_start) & (prescriptions_fl[\"Year\"] <= fl_end)]\n",
    "prescriptions_wa = prescriptions_wa[(prescriptions_wa[\"Year\"] >= wa_start) & (prescriptions_wa[\"Year\"] <= wa_end)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cause of death - broken down by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_fl = cause_of_death.copy()\n",
    "deaths_tx = cause_of_death.copy()\n",
    "deaths_wa = cause_of_death.copy()\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"StateName\"] == \"Florida\") | (deaths_fl[\"State\"].isin(fl_states))]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"StateName\"] == \"Texas\") | (deaths_tx[\"State\"].isin(tx_states))]\n",
    "deaths_wa = deaths_wa[(deaths_wa[\"StateName\"] == \"Washington\") | (deaths_wa[\"State\"].isin(wa_states))]\n",
    "\n",
    "deaths_fl = deaths_fl[(deaths_fl[\"Year\"] >= fl_start) & (deaths_fl[\"Year\"] <= fl_end)]\n",
    "deaths_tx = deaths_tx[(deaths_tx[\"Year\"] >= tx_start) & (deaths_tx[\"Year\"] <= tx_end)]  \n",
    "deaths_wa = deaths_wa[(deaths_wa[\"Year\"] >= wa_start) & (deaths_wa[\"Year\"] <= wa_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013.0    2681681\n",
       "2011.0    2656850\n",
       "2012.0    2654508\n",
       "2010.0    2562310\n",
       "2014.0    2433600\n",
       "2009.0    2419344\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prescriptions_wa.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_52032/2020115255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mprescriptions_fl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mprescriptions_wa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwa_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwa_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdeaths_fl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfl_end\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# final assert to check years\n",
    "\n",
    "assert prescriptions_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert prescriptions_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))\n",
    "\n",
    "assert deaths_fl.Year.unique().tolist() == list(range(fl_start, fl_end + 1))\n",
    "assert deaths_tx.Year.unique().tolist() == list(range(tx_start, tx_end + 1))\n",
    "assert deaths_wa.Year.unique().tolist() == list(range(wa_start, wa_end + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export all to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prescriptions_fl.to_csv(\"20_intermediate_files/prescriptions_fl.csv\", index=False)\n",
    "#prescriptions_wa.to_csv(\"20_intermediate_files/prescriptions_wa.csv\", index=False)\n",
    "\n",
    "deaths_fl.to_csv(\"20_intermediate_files/deaths_fl.csv\", index=False)\n",
    "deaths_tx.to_csv(\"20_intermediate_files/deaths_tx.csv\", index=False)\n",
    "deaths_wa.to_csv(\"20_intermediate_files/deaths_wa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions_fl.to_parquet('20_intermediate_files/prescriptions_fl.parquet', engine='fastparquet', row_group_offsets=2_500_000)\n",
    "prescriptions_wa.to_parquet('20_intermediate_files/prescriptions_wa.parquet', engine='fastparquet', row_group_offsets=2_500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for the group\n",
    "\n",
    "- may need to filter out a couple more columns - haven't done this yet as I don't want to accidentally delete something we need\n",
    "- overdose data is only broken down by year unless i messed something up - overdose analysis will have to be less granular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49aa7209ac0e1a36a89cb04290394fd089cb5ce56cb44c9d4652c0180c6152a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
